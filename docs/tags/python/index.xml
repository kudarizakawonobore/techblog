<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>python on 下り坂を昇るブログ</title>
    <link>https://kudarizakawonobore.github.io/techblog/tags/python/</link>
    <description>Recent content in python on 下り坂を昇るブログ</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Thu, 20 Feb 2020 23:09:00 +0900</lastBuildDate>
    
	<atom:link href="https://kudarizakawonobore.github.io/techblog/tags/python/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Raspberry Pi で QR コードリーダーからプログラムを動かす</title>
      <link>https://kudarizakawonobore.github.io/techblog/blog/2020/02_qrcode/</link>
      <pubDate>Thu, 20 Feb 2020 23:09:00 +0900</pubDate>
      
      <guid>https://kudarizakawonobore.github.io/techblog/blog/2020/02_qrcode/</guid>
      <description>概要 市販のQRコードリーダーを起動契機にして、何かしらの処理を行うデバイスを作りたいときの実装メモ。
環境 Raspberry Pi 3 OS: Raspbian Python: 3.6.5
知っておきたいこと USB タイプの QR コードリーダーや、バーコードリーダーは、キーボードと同様の入力デバイスとして扱うことができる。
QRコードリーダーを RaspberryPi に差した状態で、QR コードを Read する（＝デコードする）と、市販の QRコードリーダーがテキスト入力として PC にイベントを送ってくる。 なので、単に値を解読したいだけなら、テキストエディタを開いた状態で、QRコードを読み込んであげるだけでよい。
自作のアプリから、デコードした値を処理したいなら、キーボードのイベントをListen し、Enterキーが押されたのを起動契機に処理を実行するプログラムを書いてあげればよい。
候補 Python でキーボードの入力を取得するためのライブラリはいくつかあるが、今回は keyboard を採用した。
下記は、不採用になったものを含めたキーボードの入力を取得するためのライブラリ。
pynput(不採用) キーボードやマウスの入力を Listen できるライブラリ。 keyboard 同様、文字列として入力を受け付けられるが、 SSH 経由でバックグラウンド実行させることができなさそうなので、不採用。
envdev(不採用) pynput と異なり、SSH 経由でバックグラウンド実行させられる他、デバイスを指定することができる。 一方で、発生したイベントがコード文字列(KEY_A や KEY_B など）になってしまうためコードを文字列に戻してやる処理が必要になる。
keyboard(採用) バックグラウンドで実行でき、文字列をわりと簡単に処理できるので、採用。
インストール pip で keyboard をインストールする
pip install keyboard実装 import keyboardimport timeqr = &amp;#34;&amp;#34;def key_press(key):global qrif key.</description>
    </item>
    
    <item>
      <title>iPhone のカメラで撮影した画像を直接PC に取り込む</title>
      <link>https://kudarizakawonobore.github.io/techblog/blog/2020/01-capture/</link>
      <pubDate>Sun, 16 Feb 2020 20:58:40 +0900</pubDate>
      
      <guid>https://kudarizakawonobore.github.io/techblog/blog/2020/01-capture/</guid>
      <description>動機 Jupyter/OpenCV などで画像処理のロジックを書いていると、主に閾値調整やフィルタリング処理が、テスト用に使っている画像に局所最適化されてしまうことがままある。
ある程度ロジックを書いた後、複数の写真を撮ってテストできれば、調整もたやすく、コードの汎用性も上がるはず。
一方で、iPhone の画像は、何枚か撮影して、メールや iCloud, Google Photo などを使えばインターネット越しで共有できるが、そうすると、ファイル名がわかりづらかったり、Zip を解凍したりと、いくつか面倒な作業が発生する。
せっかく手元に iPhone があるのだから、直接取り込んであげられれば、作業もはかどりそうなものである。
結論 今回は外部のツールを使うだけなので、いたってシンプル。 iVCam という iPhone のアプリを使うだけ。
準備 スマホ側と、PC 側にアプリをインストールするだけ。
スマホ側 iVcam で検索。 無償版は、ファイル内にロゴが映りこむため、有償版を使った方が良い。
PC 版 こちらのサイトからダウンロードして、インストールする。 https://www.e2esoft.com/ivcam/
実行するとき PC,スマホの両方でアプリを起動している状態で、かつ、同一ネットワーク上にいると、iPhone のカメラに映っている映像がそのままPC でキャプチャ、保存できる。
撮影した画像は、アプリ側で任意のディレクトリに保存できる。 機械学習用に、アノテーション付きでファイルを管理したい場合は、撮影するタイミングでディレクトリ分けができるので、画像の管理がかなりラクになる。
利点 無線ネットワーク越しにファイルを取り込めるので、例えば、有線のWebカメラのように物理的な範囲の制限がないのが魅力。 例えば、スマホを3脚に固定しておいて、定点で複数枚写真を撮るような要件にも対応できる。
欠点 解像度はそんなに高くない模様。そのため、OCR に使おうと思うと、いまいち文字が認識しづらかったりする。</description>
    </item>
    
  </channel>
</rss>